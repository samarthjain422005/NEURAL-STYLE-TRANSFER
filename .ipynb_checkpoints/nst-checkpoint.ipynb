{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40f05e1",
   "metadata": {},
   "source": [
    "## import Libraries and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119f3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Image size: 128\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: '/Users/pushpendrammishra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 195\u001b[0m\n\u001b[0;32m    193\u001b[0m style_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(working_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(working_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 195\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    197\u001b[0m content_images \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(content_dir)\n\u001b[0;32m    198\u001b[0m style_images \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(style_dir)\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: '/Users/pushpendrammishra'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "imsize = 512 if torch.cuda.is_available() else 128\n",
    "print(f\"Image size: {imsize}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((imsize, imsize)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def load_image(image_path, size=None):\n",
    "    image = Image.open(image_path)\n",
    "    if size is not None:\n",
    "        image = transforms.Resize(size)(image)\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "def get_image_size(image):\n",
    "    return image.shape[2], image.shape[3]\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone().detach().squeeze(0)\n",
    "    image = transforms.ToPILImage()(image)\n",
    "\n",
    "    # Resize the image to match the smallest dimension (width or height)\n",
    "    min_size = min(image.size)\n",
    "    transform_resize = transforms.Resize(min_size)\n",
    "    image = transform_resize(image)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "class StyleTransfer(nn.Module):\n",
    "    def __init__(self, cnn, normalization_mean, normalization_std,\n",
    "                 content_image, style_image, content_layers=None, style_layers=None):\n",
    "        super(StyleTransfer, self).__init__()\n",
    "        self.cnn = copy.deepcopy(cnn)\n",
    "        self.normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "        if content_layers is None:\n",
    "            content_layers = ['conv_4']\n",
    "        if style_layers is None:\n",
    "            style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "        self.content_layers = content_layers\n",
    "        self.style_layers = style_layers\n",
    "\n",
    "        self.model, self.content_losses, self.style_losses = self.get_model_and_losses(content_image, style_image)\n",
    "\n",
    "    def get_model_and_losses(self, content_image, style_image):\n",
    "        content_losses = []\n",
    "        style_losses = []\n",
    "        model = nn.Sequential(self.normalization)\n",
    "        i = 0\n",
    "\n",
    "        for layer in self.cnn.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                i += 1\n",
    "                name = f'conv_{i}'\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                name = f'relu_{i}'\n",
    "                layer = nn.ReLU(inplace=False)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                name = f'pool_{i}'\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                name = f'bn_{i}'\n",
    "            else:\n",
    "                raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in self.content_layers:\n",
    "                target = model(content_image).detach()\n",
    "                content_loss = ContentLoss(target)\n",
    "                model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in self.style_layers:\n",
    "                target_feature = model(style_image).detach()\n",
    "                style_loss = StyleLoss(target_feature)\n",
    "                model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "\n",
    "        for i in range(len(model) - 1, -1, -1):\n",
    "            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "                break\n",
    "\n",
    "        model = model[:(i + 1)]\n",
    "\n",
    "        return model, content_losses, style_losses\n",
    "\n",
    "    def gram_matrix(self, input):\n",
    "        a, b, c, d = input.size()\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        return G.div(a * b * c * d)\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1).to(device)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = nn.functional.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = StyleTransfer.gram_matrix(self, target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = StyleTransfer.gram_matrix(self, input)\n",
    "        self.loss = nn.functional.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "def run_style_transfer(style_transfer, input_img, num_steps=300,\n",
    "                       style_weight=1000000, content_weight=1):\n",
    "     \n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            input_img.data.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            style_transfer.model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_transfer.style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in style_transfer.content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            sl=style_score\n",
    "            cl=content_score\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f\"run {run[0]}:\")\n",
    "                print(f'Style Loss : {style_score.item():4f} Content Loss: {content_score.item():4f}')\n",
    "                print()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    return input_img\n",
    "\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "\n",
    "working_dir = \"/Users/pushpendrammishra/Desktop/ neural transfer style\"\n",
    "content_dir = os.path.join(working_dir, \"content\")\n",
    "style_dir = os.path.join(working_dir, \"style\")\n",
    "output_dir = os.path.join(working_dir, \"output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "content_images = os.listdir(content_dir)\n",
    "style_images = os.listdir(style_dir)\n",
    "\n",
    "for content_image_name in content_images:\n",
    "    for style_image_name in style_images:\n",
    "        content_image_path = os.path.join(content_dir, content_image_name)\n",
    "        style_image_path = os.path.join(style_dir, style_image_name)\n",
    "        output_image_name = f\"{os.path.splitext(content_image_name)[0]}_{os.path.splitext(style_image_name)[0]}.jpg\"\n",
    "        print(\"Building model for content image \",os.path.splitext(content_image_name)[0],\" Vs style image \",os.path.splitext(style_image_name)[0])\n",
    "        content_image = load_image(content_image_path)\n",
    "        style_image = load_image(style_image_path, get_image_size(content_image))\n",
    "\n",
    "        style_transfer = StyleTransfer(cnn, cnn_normalization_mean, cnn_normalization_std, content_image, style_image)\n",
    "\n",
    "        input_image = content_image.clone()\n",
    "\n",
    "        output_image = run_style_transfer(style_transfer, input_image)\n",
    "\n",
    "       \n",
    "        output_image_path = os.path.join(output_dir, output_image_name)\n",
    "        output_image_pil = transforms.ToPILImage()(output_image.squeeze(0).cpu())\n",
    "        output_image_pil.save(output_image_path)\n",
    "\n",
    "        print(f\"Saved stylized image: {output_image_path}\")\n",
    "\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1dae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for content_image_name in content_images:\n",
    "    content_image_path = os.path.join(content_dir, content_image_name)\n",
    "    content_image = load_image(content_image_path)\n",
    "    \n",
    "    plt.figure(figsize=(100, 5 * len(style_images)))\n",
    "    \n",
    "    for idx, style_image_name in enumerate(style_images, start=1):\n",
    "        style_image_path = os.path.join(style_dir, style_image_name)\n",
    "        style_image = load_image(style_image_path, get_image_size(content_image))\n",
    "        output_image_name = f\"{os.path.splitext(content_image_name)[0]}_{os.path.splitext(style_image_name)[0]}.jpg\"\n",
    "        output_image_path = os.path.join(output_dir, output_image_name)\n",
    "        output_image = load_image(output_image_path)  # Load the output image\n",
    "        \n",
    "        # Calculate the subplot indices to ensure they are in rows\n",
    "        row_index = (idx - 1) * 3\n",
    "        \n",
    "        # Display content image in the first column of the current row\n",
    "        plt.subplot(len(style_images), 3, row_index + 1)\n",
    "        imshow(content_image, title='Content Image')\n",
    "        \n",
    "        # Display style image in the second column of the current row\n",
    "        plt.subplot(len(style_images), 3, row_index + 2)\n",
    "        imshow(style_image, title='Style Image')\n",
    "        \n",
    "        # Display output image in the third column of the current row\n",
    "        plt.subplot(len(style_images), 3, row_index + 3)\n",
    "        imshow(output_image, title='Output Image')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe3d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
